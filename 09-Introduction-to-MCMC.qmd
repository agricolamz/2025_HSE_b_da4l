---
editor_options: 
  chunk_output_type: console
---

# Введение в Марковские цепи Монте-Карло

```{r setup08}
#| include: false

knitr::opts_chunk$set(echo = TRUE, message = FALSE)
library(tidyverse)
theme_set(theme_bw())
```

```{r}
library(tidyverse)
```

Марковская цепь Монте-Карло (Markov chain Monte Carlo, MCMC) --- это класс алгоритмов для семплирования, которые позволяют моделировать некоторое распределение вероятностей. При моделировании используют разные алгоритмы, мы будем смотреть на примере алгоритма Метрополиса-Гастингса (Metropolis-Hastings).

Для того, чтобы в этом разобраться нам потребуется обсудить:

- метод Монте-Карло;
- марковские цепи;
- алгоритма Метрополиса-Гастингса.

## Марковские цепи

Марковский процесс

- конечное количество состояний
- вероятность переходов из одного состояния в другое

Возьмем наш датасет с sms и посмотрим частоты разных частей речи::

```{r}
#| echo: false

sms_pos <- read_csv("https://raw.githubusercontent.com/agricolamz/2022_da4l/master/data/spam_sms_pos_lead_pos.csv")
library(tidytext)
sms_pos |> 
  count(pos, type) |> 
  mutate(pos = reorder_within(pos, n, type)) |> 
  ggplot(aes(n, pos))+
  geom_col(fill = "lightblue")+
  facet_wrap(~type, scales = "free")+
  scale_y_reordered()+
  labs(title = "Частотность разных частей речи в sms сообщениях",
       x = "", y = "",
       caption = "kaggle.com/uciml/sms-spam-collection-dataset")
```

Теперь давайте посмотрим на частотность переходов из одних состояний в другие:

```{r}
#| echo: false
#| fig-width: 12

sms_pos |> 
  na.omit() |> 
  count(type, pos, pos2) |> 
  mutate(pos = reorder_within(pos, -n, type),
         pos2 = reorder_within(pos2, -n, type)) |> 
  group_by(type) |> 
  mutate(ratio = n/sum(n)) |> 
  ggplot(aes(pos, pos2, size = ratio, fill = ratio))+
  geom_raster()+
  geom_point(show.legend = FALSE)+
  scale_fill_gradient(low="grey95", high="red")+
  facet_wrap(~type, scales = "free")+
  scale_x_reordered()+
  scale_y_reordered()+
  theme(legend.position = "bottom")
```

Иногда это визуализируют при помощи графов, но в нашем случае (это граф ham) это достаточно бесполезно (наводите на вершины стрелочки, чтобы что-то разглядеть):

```{r}
#| echo: false

sms_pos |> 
  filter(type == "ham") |> 
  na.omit() |> 
  count(pos, pos2) |> 
  mutate(perc = round(n/sum(n)*100, 2),
         from_id = as.double(factor(pos)),
         to_id = as.double(factor(pos2))) ->
  ham_graph

library(DiagrammeR)

nodes_ham <- create_node_df(length(unique(c(ham_graph$pos, ham_graph$pos2))),
                            label = sort(unique(c(ham_graph$pos, ham_graph$pos2))),
                            width = 0.2,
                            fontsize = 4,
                            penwidth = 0.1)
edge_ham <- create_edge_df(from = ham_graph$from_id, 
                           to = ham_graph$to_id, 
                           penwidth = 0.3,
                           fontsize = 2,
                           arrowsize = 0.4,
                           arrowhead = "vee",
                           tooltip = str_c(ham_graph$pos, 
                                           "➝",
                                           ham_graph$pos2,
                                           " ",
                                           ham_graph$perc))
create_graph(nodes_ham, 
             edge_ham) |> 
  render_graph(layout = "circle")
```


Можно посмотреть на [славную визуализацию](http://setosa.io/blog/2014/07/26/markov-chains/index.html) (спасибо за ссылку Марине Дубовой).

## Хакерская статистика (основано на ["Статистика для хакеров" Джейка Вандерпласа](https://www.youtube.com/watch?v=Iq9DzN6mvYA))

Вообще симуляции позволяют делать статистику более осмысленной.

### Биномиальные данные

::: {.callout-note}
Немного упрощая данные из статьи [@rosenbach03: 394], можно сказать что носители британского английского предпочитают *s*-генитив (90%) *of*-генитиву (10%). Можно ли сказать, что перед Вами носитель британского английского, если Вы наблюдаете в интервью актера из 120 контекстов 100 *s*-генитивов?
:::

Вероятность получить 100 успехов из 120 случаев, если мы верим, что вероятность успеха равна 0.9 описывается биномиальным распределением:

$$P(H = h|p, n) = \binom{n}{h}\times p^h\times(1-p)^{1-h}$$

**Фреквентистский подход**: биномиальный тест

- H$_0$ человек говорит *s*-генитив с вероятностью 0.9
- α = 0.05

```{r}
#| echo: false

tibble(h = 0:120,
       p = dbinom(x = h, size = 120, prob = 0.9)) |> 
  ggplot(aes(h, p))+
  geom_line()+
  geom_area(aes(x = ifelse(h>=qbinom(0.025, 120, 0.9) &
                           h<=qbinom(0.975, 120, 0.9), h, NA)), fill = "darkgreen")+
  geom_vline(xintercept = 100, linetype = 2)+
  scale_x_continuous(breaks = c(0:9*20))
```

**Байесовский подход**: биномиальная функция правдоподобия перемножается с априорным бета распределением чтобы получить апостериорное распределение.

```{r}
tibble(h = seq(0, 1, 0.01),
       y = dbeta(h, shape1 = 100 + 90, shape2 = 20 + 10)) |> 
  ggplot(aes(h, y))+
  geom_line()+
  geom_area(aes(x = ifelse(h>=qbeta(0.025, shape1 = 100 + 90, shape2 = 20 + 10) &
                           h<=qbeta(0.975, shape1 = 100 + 90, shape2 = 20 + 10), h, NA)), fill = "darkgreen")+
  geom_vline(xintercept = 100/120, linetype = 2)
```

**Хакерский подход**: симуляция:

```{r}
set.seed(42)
map_dbl(1:1000, function(i){
  sample(0:1, 120, replace = TRUE, prob = c(0.1, 0.9)) |> 
    sum()
}) ->
  simulations

tibble(sum = simulations) |> 
  mutate(greater = sum <= 100) |> 
  group_by(greater) |> 
  summarise(number = n())

tibble(sum = simulations) |> 
  ggplot(aes(sum))+
  geom_density()+
  geom_vline(xintercept = 100, linetype = 2)+
  scale_x_continuous(breaks = c(0:9*20), limits = c(0, 120))
```

Аналогично можно использовать:

- случайное перемешивание вместо двухвыборочного t-теста;
- бутстрэп вместо одновыборочного t-теста.

### Метод Монте-Карло

Группа методов изучения случайных процессов, которые базируются на:

- возможности производить бесконечного количества случайных значений
- для известных или новых распределений

Представим себе, что у нас есть какой-то интеграл, который мы хотим посчитать. Например такой:

```{r}
tibble(x = seq(0, 1, length.out = 1000)) |> 
  ggplot(aes(x))+
  stat_function(fun = function(x){x^12-sin(x)+1}, geom = "area", fill = "lightblue")
```

Мы можем насэмплировать точек из комбинации двух унимодальных распределений u(0, 1), и u(0, 1.2) и посмотреть, кто попадает в область, а кто нет:

```{r}
set.seed(42)
tibble(x = runif(1e3, 0, 1), 
       y = runif(1e3, 0, 1.2),
       in_area = y < x^12-sin(x)+1) |> 
  ggplot(aes(x, y))+
  stat_function(fun = function(x){x^12-sin(x)+1}, geom = "area", fill = "lightblue")+
  geom_point(aes(color = in_area), show.legend = FALSE)
```

Сколько попало?

```{r}
set.seed(42)
tibble(x = runif(1e3, 0, 1), 
       y = runif(1e3, 0, 1.2),
       in_area = y < x^12-sin(x)+1) |> 
  count(in_area)

528/1000*1.2

integrate(function(x){x^12-sin(x)+1}, 0, 1)
```

А если увеличить количество наблюдений?
```{r}
#| cache: true

set.seed(42)
tibble(x = runif(1e7, 0, 1), 
       y = runif(1e7, 0, 1.2),
       in_area = y < x^12-sin(x)+1) |> 
  count(in_area)

5143667/10000000*1.2

integrate(function(x){x^12-sin(x)+1}, 0, 1)
```

## Соединение идей Марковских цепей и Монте-Карло

```{r}
#| eval: false

shiny::runGitHub("agricolamz/mcmc_shiny")
```

Основные проблемы MCMC:

- Зависимость от начального значения. Решение: выкинуть начальную часть цепи (burn-in).
- Полученные значения автокоррелируют, так как они были получины при помощи марковского процесса. Решение: брать, например, каждое третье значение.

Вы можете почитать историю идей MCMC в работе [@robert11] ([доступна здесь](https://arxiv.org/pdf/0808.2902.pdf)).

## `brms`

Для вычисления всяких сложных статистических моделей люди придумали вероятностные языки программирования. Чаще всего они являются расширением для стандартных языков программирования, но иногда становятся самодостаточными языками (однако они все равно написаны на каких-то быстрых языках программирования типа C++). Примерами таких самодостаточных языков является:

- BUGS (Bayesian inference Using Gibbs Sampling)
- JAGS (Just Another Gibbs Sampler)
- NIMBLE
- NUTS (No-U-Turn-Sampler)
- Stan

Для них пишут обертки на разных языках, мы будем использовать пакет `brms`, который является оберткой над пакетом `rstan`, который является оберткой для Stan. Вы можете установить эти пакеты к себе на компьютер, но есть высокая вероятность, что что-то пойдет не так, в связи с чем, я предлагаю всем использовать rstudio.cloud для примеров на `brms`.

Чтобы повторить примеры Вам нужно:

- Установить на свой компьютер `rstan` ([инструкции](https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started)) и `brms`

Если на вашем компьютере не выходит, попробуйте <posit.cloud>

### Регрессионный пример

::: {.callout-note}
В работе [@coretta2016] собраны [данные](https://raw.githubusercontent.com/agricolamz/2022_da4l/master/data/Coretta_2017_icelandic.csv) длительности исландских гласных. Отфильтруйте данные, произнесенные носителем `tt01` (переменная `speaker`), и смоделируйте длительность гласных (переменная `vowel.dur`) нормальным распределением используя в качестве априорного распределения нормальное распределение со средним 275 и стандартным отклонением 65 (основано на [@hillenbrand95]). 
:::

```{r}
read_csv("https://raw.githubusercontent.com/agricolamz/2024_HSE_b_da4l/master/data/Coretta_2017_icelandic.csv") |> 
  filter(speaker == "tt01") ->
  vowel_data
```

В качестве первого этапа стоит сформулировать модель и посмотреть, в какой форме от нас ожидают априорное распределение.

```{r}
library(brms)
get_prior(vowel.dur ~ 0 + Intercept, 
          family = "normal",
          data = vowel_data)
```

Сделаем нашу первую модель:

```{r first_brm}
#| cache: true

normal_fit <- brm(vowel.dur ~ 0 + Intercept, 
                  family = "normal",
                  data = vowel_data,
                  prior = c(prior(normal(275, 65), coef = Intercept)),
                  silent = TRUE)
normal_fit
```

Визуализируем нашу модель:

```{r}
plot(normal_fit)
```

После того как мы сгенерировали наши распределения, мы можем их достать и что-нибудь посчитать:

```{r}
as_draws_array(normal_fit) |> 
  posterior::summarize_draws()
as_draws_rvars(normal_fit)
```

Можно визуально оценить, где лежат наши данные, а что предсказывает полученная модель:

```{r}
pp_check(normal_fit)
```
